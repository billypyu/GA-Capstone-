{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Web-Scraping-with-Selenium\" data-toc-modified-id=\"Web-Scraping-with-Selenium-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Web Scraping with Selenium</a></span><ul class=\"toc-item\"><li><span><a href=\"#Learning-Objectives\" data-toc-modified-id=\"Learning-Objectives-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Learning Objectives</a></span></li><li><span><a href=\"#Installs\" data-toc-modified-id=\"Installs-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Installs</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Required-Software\" data-toc-modified-id=\"Required-Software-1.2.0.1\"><span class=\"toc-item-num\">1.2.0.1&nbsp;&nbsp;</span>Required Software</a></span></li><li><span><a href=\"#Python\" data-toc-modified-id=\"Python-1.2.0.2\"><span class=\"toc-item-num\">1.2.0.2&nbsp;&nbsp;</span>Python</a></span></li></ul></li></ul></li><li><span><a href=\"#Using-Selenium-to-Automate-Logins\" data-toc-modified-id=\"Using-Selenium-to-Automate-Logins-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Using Selenium to Automate Logins</a></span><ul class=\"toc-item\"><li><span><a href=\"#Headless-Browsing\" data-toc-modified-id=\"Headless-Browsing-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Headless Browsing</a></span></li><li><span><a href=\"#Logging-in-to-GHE\" data-toc-modified-id=\"Logging-in-to-GHE-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Logging in to GHE</a></span></li><li><span><a href=\"#Passing-Cookies-to-Requests\" data-toc-modified-id=\"Passing-Cookies-to-Requests-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>Passing Cookies to Requests</a></span></li></ul></li><li><span><a href=\"#Using-Selenium-to-Load-Javascript-Pages\" data-toc-modified-id=\"Using-Selenium-to-Load-Javascript-Pages-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Using Selenium to Load Javascript Pages</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#This-function-defines-my-driver-for-a-single-page\" data-toc-modified-id=\"This-function-defines-my-driver-for-a-single-page-1.4.0.1\"><span class=\"toc-item-num\">1.4.0.1&nbsp;&nbsp;</span>This function defines my driver for a single page</a></span></li><li><span><a href=\"#This-function-allows-me-to-check-that-my-page-has-finished-loading\" data-toc-modified-id=\"This-function-allows-me-to-check-that-my-page-has-finished-loading-1.4.0.2\"><span class=\"toc-item-num\">1.4.0.2&nbsp;&nbsp;</span>This function allows me to check that my page has finished loading</a></span></li><li><span><a href=\"#Setting-up-the-lists-of-items-that-I-need-to-collect\" data-toc-modified-id=\"Setting-up-the-lists-of-items-that-I-need-to-collect-1.4.0.3\"><span class=\"toc-item-num\">1.4.0.3&nbsp;&nbsp;</span>Setting up the lists of items that I need to collect</a></span></li><li><span><a href=\"#Define-max-pages\" data-toc-modified-id=\"Define-max-pages-1.4.0.4\"><span class=\"toc-item-num\">1.4.0.4&nbsp;&nbsp;</span>Define max pages</a></span></li><li><span><a href=\"#Create-user-tuples\" data-toc-modified-id=\"Create-user-tuples-1.4.0.5\"><span class=\"toc-item-num\">1.4.0.5&nbsp;&nbsp;</span>Create user tuples</a></span></li><li><span><a href=\"#Write-tuples-to-CSV\" data-toc-modified-id=\"Write-tuples-to-CSV-1.4.0.6\"><span class=\"toc-item-num\">1.4.0.6&nbsp;&nbsp;</span>Write tuples to CSV</a></span></li><li><span><a href=\"#Bring-it-all-together\" data-toc-modified-id=\"Bring-it-all-together-1.4.0.7\"><span class=\"toc-item-num\">1.4.0.7&nbsp;&nbsp;</span>Bring it all together</a></span></li><li><span><a href=\"#Here,-we'll-grab-all-the-ratings-for-a-single-game\" data-toc-modified-id=\"Here,-we'll-grab-all-the-ratings-for-a-single-game-1.4.0.8\"><span class=\"toc-item-num\">1.4.0.8&nbsp;&nbsp;</span>Here, we'll grab all the ratings for a single game</a></span></li><li><span><a href=\"#A-final-wrapper-function\" data-toc-modified-id=\"A-final-wrapper-function-1.4.0.9\"><span class=\"toc-item-num\">1.4.0.9&nbsp;&nbsp;</span>A final wrapper function</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping with Selenium\n",
    "*Author: Douglas Strodtman (SaMo)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "1. Scraping Javascript pages\n",
    "2. Manipulating objects on a page\n",
    "3. Automating logins\n",
    "4. Passing cookies to `requests`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required Software\n",
    "\n",
    "- Google Chrome\n",
    "- [Xpath Helper](https://chrome.google.com/webstore/detail/xpath-helper/hgimnogjllphhhkhlmebbmlgjoejdpjl?hl=en)\n",
    "- [Chromedriver](http://chromedriver.chromium.org/downloads) (Download the Chromedriver for your OS. Unzip and move the `chromedriver` file to the directory containing this notebook.)\n",
    "\n",
    "#### Python \n",
    "- scrapy\n",
    "- selenium\n",
    "- beautiful soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment and run the install for any of the packages you're missing below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scrapy\n",
    "# !pip install selenium \n",
    "# !pip install beautifulsoup4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from scrapy.selector import Selector\n",
    "import re\n",
    "import time\n",
    "import csv\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Selenium to Automate Logins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid typing my password in the browser, I created a hidden file with the variable\n",
    "\n",
    "```PASSWORD = 'myPassw0rd'``` \n",
    "\n",
    "defined. I add this file to my `.gitignore` so it doesn't get accidentally shared when I push to Github. **I'm not saying this is security best practices, but it's a simple safeguard.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:File `'.password.py'` not found.\n"
     ]
    }
   ],
   "source": [
    "%run .password.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Headless Browsing\n",
    "One of the great options available with Selenium is the ability to automate websurfing without open a visual browser. This is as simple as adding\n",
    "\n",
    "```options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging in to GHE\n",
    "All uses of Selenium are esoteric. If you want to log in to a site, build out a custom function that utilizes that site's structure and functionality and return the logged in driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def login_to_github(username, user_pass, headless=False, repo='DSI-US-4/course-info'):\n",
    "    if headless:\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('headless')\n",
    "        driver = webdriver.Chrome('./chromedriver', chrome_options=options)\n",
    "    else:\n",
    "        driver = webdriver.Chrome('./chromedriver')\n",
    "    driver.get(f'https://git.generalassemb.ly/{repo}')\n",
    "    \n",
    "    \n",
    "\n",
    "    user = driver.find_elements_by_css_selector('input[type=text]')[0]\n",
    "    user.send_keys(username)\n",
    "\n",
    "    password = driver.find_element_by_css_selector('input[type=password]')\n",
    "    password.send_keys(user_pass)\n",
    "\n",
    "    button = driver.find_element_by_css_selector('.btn')\n",
    "    button.click()\n",
    "    \n",
    "    \n",
    "\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PASSWORD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-57bc439b692b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdriver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogin_to_github\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dstrodtman'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPASSWORD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'PASSWORD' is not defined"
     ]
    }
   ],
   "source": [
    "driver = login_to_github('dstrodtman', PASSWORD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this opens a new Chrome browser window. I can use xpath (or other options) to select elements of this page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "insights = driver.find_element_by_xpath(\"//a[@class='js-selected-navigation-item reponav-item'][3]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Insights'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insights.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can now automate a click to navigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "insights.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that I've now navigated to a different page in my browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing Cookies to Requests\n",
    "While Selenium is powerful, it can be much slower than `requests`. Whenever possible, accelerate your scraping by capturing the html source instead of rendering the Javascript. You can pass cookies that are generated automatically back to `requests` to mimic browser behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cookie_jar(driver):\n",
    "    cookies = driver.get_cookies()\n",
    "    cookie_jar = {x['name']:x['value'] for x in cookies}\n",
    "    \n",
    "    return cookie_jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cookie_jar = get_cookie_jar(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_fi_sess': 'eyJsYXN0X3dyaXRlIjoxNTI5NDI1NTA2MTk4LCJzZXNzaW9uX2lkIjoiYThjY2Q0OGU1NDE2ZDFjZWMwMmIyNDY0ODk2Y2JjZmQiLCJsYXN0X3JlYWRfZnJvbV9yZXBsaWNhcyI6MTUyOTQyNTUyMTE3NSwic3B5X3JlcG8iOiJEU0ktVVMtNC9jb3Vyc2UtaW5mbyIsInNweV9yZXBvX2F0IjoxNTI5NDI1NTIwfQ%3D%3D--a524b550112b743f242a2c915ed6682455d48e53',\n",
       " '_gh_render': 'BAh7B0kiD3Nlc3Npb25faWQGOgZFVEkiRTJhYzRjNGRhZWQyYWU3ZDVkNzky%0AYWQ4YmY4Yzc5YTVlNjdlNmVkODgyZTM5MDc3MDM0Yjg4NWU2YmRmNjMyOTkG%0AOwBGSSIPdXNlcl9sb2dpbgY7AEZJIg9kc3Ryb2R0bWFuBjsAVA%3D%3D%0A--307ecb8caa23da7a65643a4ba1bf0cc598edd689',\n",
       " 'tz': 'America%2FLos_Angeles',\n",
       " 'dotcom_user': 'dstrodtman',\n",
       " 'logged_in': 'yes',\n",
       " '__Host-user_session_same_site': 'EoNbv2WD56BErIcQIL_716Ia5e33-Fop4uSYD3CYkY40MhNY',\n",
       " 'user_session': 'EoNbv2WD56BErIcQIL_716Ia5e33-Fop4uSYD3CYkY40MhNY'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cookie_jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I pass these cookies back to requests using the `cookies` arg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get('https://git.generalassemb.ly/DSI-US-4/course-info', \n",
    "                    cookies=cookie_jar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this simple example, I'll just extract the total number of commits to this page (here, I use `Selector` from `scrapy` to select by xpath so that my process for using `requests` is more similar to that for `selenium`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "commits = Selector(text=page.text).xpath(\"//li[@class='commits']/a/span\").extract()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<span class=\"num text-emphasized\">\\n                526\\n              </span>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, my result isn't as clean as I might like, but with some quick `BeautifulSoup` and regex parsing, I can clean things up. **In general**, my preferenece is to get as much data as I can in a single call and then clean it using the same reusable functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'526'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('\\s*', '', BeautifulSoup(commits, 'html.parser').text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Selenium to Load Javascript Pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below functions are esoteric to boardgamegeek.com, which is the website that I chose to scrape for my capstone project. My approach was to maximize the amount of navigation I could do with `requests` and only use `selenium` for loading dynamically generated pages so that I could capture the data from their page source. There are **many** other ways to approach these problems, and I'm not suggesting that my solution is the best, just that it is a workable solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This function defines my driver for a single page\n",
    "I choose to open a new driver for each page that I visit, rather than automating button clicks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_bgg(glink, curr_page, headless=True):\n",
    "    if headless:\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('headless')\n",
    "        driver = webdriver.Chrome('./chromedriver', chrome_options=options)\n",
    "    else:\n",
    "        driver = webdriver.Chrome('./chromedriver')\n",
    "    driver.get(f'https://boardgamegeek.com{glink}/ratings?pageid={curr_page}&rated=1')\n",
    "\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This function allows me to check that my page has finished loading\n",
    "I was running into issues where I was trying to scrape before the page had finished loading, which at various times in my troubleshooting led to either just errors or (in an earlier iteration of this approach) scraping the results from the previous page that I had visited. While this function is all error handling, I'm programming around expected error behavior based upon what I know about the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_page_loaded(driver, last_page_el):\n",
    "    first_rater = []\n",
    "    page_el = []\n",
    "    while not first_rater or not page_el:\n",
    "        try:\n",
    "            first_rater = driver.find_element_by_xpath(\"//ratings-module//li[@class='summary-item summary-rating-item']\\\n",
    "                                                        [1]/div[@class='comment-header']/div/div/a\")\n",
    "            page_el = first_rater.text\n",
    "        except:\n",
    "            time.sleep(.5)\n",
    "    while page_el == last_page_el:\n",
    "        try:\n",
    "            first_rater = driver.find_element_by_xpath(\"//ratings-module//li[@class='summary-item summary-rating-item']\\\n",
    "                                                        [1]/div[@class='comment-header']/div/div/a\")\n",
    "            page_el = first_rater.text\n",
    "        except:\n",
    "            time.sleep(.5)\n",
    "            \n",
    "    return page_el"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the lists of items that I need to collect\n",
    "I had previously scraped the top 10k boardgames using `requests` from ranked lists that returned simple html. I was able to capture a unique identifier number(`gid`), the specific url path to that game (`glink`), and the total number of users that had provided a rating for that game (`numrating`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gid_link_numratings(start=0, stop=10000):\n",
    "    gids = []\n",
    "    glinks = []\n",
    "    numratings = []\n",
    "    with open('data/numratings', 'r') as f:\n",
    "        reader = csv.reader(islice(f, start, stop+1))\n",
    "        for row in reader:\n",
    "            gids.append(row[0])\n",
    "            glinks.append(row[1])\n",
    "            numratings.append(row[2])\n",
    "    return gids, glinks, numratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define max pages\n",
    "To avoid having to automate clicks, I chose to leverage the design of the site to iterate through pages. Here, I find the number of pages of reviews that I should expect for each game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_max_pages(numrating):\n",
    "    numrating = int(numrating)\n",
    "    if numrating%50 != 0:\n",
    "        max_pages = numrating//50 + 1\n",
    "    else:\n",
    "        max_pages = int(numrating/50)\n",
    "    return max_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create user tuples\n",
    "Because I knew that I wanted my data to end up in a normalized postgres database, I chose to include all necessary info here to define my table. Both `user_name` and `gid` are unique identifiers, so I use these to index ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_user_rows(user_names, gid, ratings):\n",
    "    return list(zip(user_names, [gid]*len(user_names), ratings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write tuples to CSV\n",
    "I write out my data after **every** query into a file for each game. Reasons:\n",
    "- For my scrape, I knew that I needed to wait 2s between calls so as to not overload the site\n",
    "- This avoids the possibility of filling up my RAM and crashing my instance\n",
    "- I cannot lose any data to unknown errors (at most, I only lose a single page query worth of data)\n",
    "- My files will be of expected len (my max `numratings` for a game is only around 70k, so my files won't get huge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_user_rows(gid,user_rows):\n",
    "    with open(f'data/{gid}_users', 'a+') as f:\n",
    "        csv.writer(f).writerows(user_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bring it all together\n",
    "Here I build out a function to combine these smaller functions to grab all the ratings for a single game. Note that I have both a log file and I print out my progress along the way. This helps me in knowing when my attempts fail (as they certainly will)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_game_raters(gid, glink, max_pages, curr_page=1):\n",
    "\n",
    "    last_page_el = []\n",
    "\n",
    "    while curr_page <= max_pages:\n",
    "        start = time.time()\n",
    "\n",
    "        driver = connect_to_bgg(glink, curr_page)\n",
    "        \n",
    "        last_page_el = check_page_loaded(driver, last_page_el)\n",
    "        \n",
    "        html = driver.page_source\n",
    "        \n",
    "        driver.quit()\n",
    "\n",
    "        xpath_user_names = \"//ratings-module//div[@class='comment-header']/div/div/a/text()\"\n",
    "        xpath_user_links = \"//ratings-module//div[@class='comment-header']/div/div/a/@href\"\n",
    "        xpath_ratings = \"//ratings-module//li/div[@class='summary-item-callout']/div/text()\"\n",
    "\n",
    "        user_names = Selector(text=html).xpath(xpath_user_names).extract()\n",
    "        user_links = Selector(text=html).xpath(xpath_user_links).extract()\n",
    "        dirty_ratings = Selector(text=html).xpath(xpath_ratings).extract()\n",
    "\n",
    "        ratings = []\n",
    "        for rating in dirty_ratings:\n",
    "            ratings.append(re.sub('\\s', '', rating))\n",
    "\n",
    "        user_rows = make_user_rows(user_names, gid, ratings)\n",
    "\n",
    "        write_user_rows(gid,user_rows)\n",
    "        \n",
    "        print(f'Scraped {gid} page {curr_page} of {max_pages} in {time.time()-start}s')\n",
    "        \n",
    "        curr_page += 1\n",
    "\n",
    "    with open('get_users_log', 'a+') as f:\n",
    "        f.write(f'{time.time()} {gid} finished\\n')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, we'll grab all the ratings for a single game\n",
    "My `gids` are ordered by `numratings`, so I choose a game a little further in so this doesn't take forever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "gids, glinks, numratings = read_gid_link_numratings()\n",
    "\n",
    "glink = glinks[8000]\n",
    "gid = gids[8000]\n",
    "max_pages = set_max_pages(numratings[8000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 1514 page 1 of 3 in 6.446454048156738s\n",
      "Scraped 1514 page 2 of 3 in 5.369781017303467s\n",
      "Scraped 1514 page 3 of 3 in 4.921979188919067s\n"
     ]
    }
   ],
   "source": [
    "get_game_raters(gid, glink, max_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A final wrapper function\n",
    "Here I build out a final function to wrap all of my inside functions and iterate through a set amount of games.\n",
    "\n",
    "**NOTE**: It will take roughly two weeks for this scrape to complete. I managed to cut this time to around 4 days by splitting this up onto 10 AWS instances.\n",
    "\n",
    "**In addition**, some of the paths in my `glinks` have been corrupted due to internal changes in board game names that will result in infinite while loops with my current code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_ratings(start=0, stop=10000):\n",
    "    gids, glinks, numratings = read_gid_link_numratings(start, stop)\n",
    "    for gid, glink, numrating in zip(gids, glinks, numratings):\n",
    "        max_pages = set_max_pages(numrating)\n",
    "        get_game_raters(gid, glink, max_pages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
