{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-Cleaning:--Earning-Call-Transcripts\" data-toc-modified-id=\"Data-Cleaning:--Earning-Call-Transcripts-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data Cleaning:  Earning Call Transcripts</a></span><ul class=\"toc-item\"><li><span><a href=\"#Retrieve-all-txt-files-from-directories\" data-toc-modified-id=\"Retrieve-all-txt-files-from-directories-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Retrieve all txt files from directories</a></span></li><li><span><a href=\"#Extract-text-from-each-text-files-by-filename\" data-toc-modified-id=\"Extract-text-from-each-text-files-by-filename-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Extract text from each text files by filename</a></span></li><li><span><a href=\"#Create-Date-Frame\" data-toc-modified-id=\"Create-Date-Frame-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Create Date Frame</a></span></li><li><span><a href=\"#Split-transcripts-into-management-discussion-and-Q&amp;A-session\" data-toc-modified-id=\"Split-transcripts-into-management-discussion-and-Q&amp;A-session-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Split transcripts into management discussion and Q&amp;A session</a></span></li><li><span><a href=\"#Extract-call-release-data,-revenue-status,-and-ESP-status\" data-toc-modified-id=\"Extract-call-release-data,-revenue-status,-and-ESP-status-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Extract call release data, revenue status, and ESP status</a></span></li></ul></li><li><span><a href=\"#Get-stock-price-and-volume-from-Yahoo-Finance\" data-toc-modified-id=\"Get-stock-price-and-volume-from-Yahoo-Finance-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Get stock price and volume from Yahoo Finance</a></span></li><li><span><a href=\"#Sentiment-Score-and-Text-Stat-for-MD-and-QA\" data-toc-modified-id=\"Sentiment-Score-and-Text-Stat-for-MD-and-QA-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Sentiment Score and Text Stat for MD and QA</a></span><ul class=\"toc-item\"><li><span><a href=\"#Text-stat-in-transcripts\" data-toc-modified-id=\"Text-stat-in-transcripts-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Text stat in transcripts</a></span></li><li><span><a href=\"#Text-stat-in-MD-&amp;-QA\" data-toc-modified-id=\"Text-stat-in-MD-&amp;-QA-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Text stat in MD &amp; QA</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import pandas_datareader as pdr\n",
    "from textblob import TextBlob\n",
    "\n",
    "import re\n",
    "from textstat.textstat import textstat \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources: \n",
    "https://github.com/shivam5992/textstat\n",
    "\n",
    "\n",
    "https://www.mckinleycapital.com/getting-sentimental-conference-call-sentiment-stock-returns/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning:  Earning Call Transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve all txt files from directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/Ford Motor (F) Q3 2018 Results - Earnings Call Transcript.txt',\n",
       " '../data/Ford Motor (F) Mark Fields on Q1 2016 Results - Earnings Call Transcript.txt',\n",
       " '../data/Tesla Motors (TSLA) Elon Reeve Musk on Q2 2015 Results - Earnings Call Transcript.txt',\n",
       " '../data/Tesla (TSLA) Q3 2017 Results - Earnings Call Transcript.txt',\n",
       " '../data/General Motors (GM) Q3 2017 Results - Earnings Call Transcript.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "filenames = []\n",
    "for filename in glob.glob('../data/*.txt'):\n",
    "    filenames.append(filename)\n",
    "filenames[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract text from each text files by filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total files loaded: 61\n"
     ]
    }
   ],
   "source": [
    "transcripts = []\n",
    "for filename in filenames: \n",
    "    with open(filename, 'r') as f:\n",
    "        cont = f.read()\n",
    "        transcripts.append(cont)\n",
    "        \n",
    "print('total files loaded:',len(transcripts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Date Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcripts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ford Motor (F) Q3 2018 Results - Earnings Call...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ford Motor (F) Mark Fields on Q1 2016 Results ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tesla Motors (TSLA) Elon Reeve Musk on Q2 2015...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tesla (TSLA) Q3 2017 Results - Earnings Call T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>General Motors (GM) Q3 2017 Results - Earnings...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         transcripts\n",
       "0  Ford Motor (F) Q3 2018 Results - Earnings Call...\n",
       "1  Ford Motor (F) Mark Fields on Q1 2016 Results ...\n",
       "2  Tesla Motors (TSLA) Elon Reeve Musk on Q2 2015...\n",
       "3  Tesla (TSLA) Q3 2017 Results - Earnings Call T...\n",
       "4  General Motors (GM) Q3 2017 Results - Earnings..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(transcripts, columns=[ 'transcripts'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split transcripts into management discussion and Q&A session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_MD = []\n",
    "text_QA = []\n",
    "\n",
    "for i in range(len(df.transcripts)): \n",
    "    text_MD.append(df.transcripts[i].split('Question-and-Answer Session')[0])\n",
    "    text_QA.append(df.transcripts[i].split('Question-and-Answer Session')[1])\n",
    "\n",
    "df['tx_MD'] = text_MD\n",
    "df['tx_QA'] = text_QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract call release data, revenue status, and ESP status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "esp_date = []\n",
    "esp_status = []\n",
    "esp_status_all = []\n",
    "esp = []\n",
    "company = []\n",
    "\n",
    "rev_status_all =[]\n",
    "rev_status = []\n",
    "rev_total=[]\n",
    "rev_pct= []\n",
    "\n",
    "for text in df['tx_MD']:\n",
    "        esp_date.append(re.findall(r'\\d{2}-\\d{2}-\\d{2}', text)[0])\n",
    "        esp_status_all.append(re.findall(r'\\n(EPS.*?\\$([\\-\\.\\d]+) (misses|beats).+?([\\-\\.\\d]+).+?)\\n', text)[0][0])\n",
    "        esp_status.append(re.findall(r'\\n(EPS.*?\\$([\\-\\.\\d]+) (misses|beats).+?([\\-\\.\\d]+).+?)\\n', text)[0][2])\n",
    "        esp.append(re.findall(r'\\n(EPS.*?\\$([\\-\\.\\d]+) (misses|beats).+?([\\-\\.\\d]+).+?)\\n', text)[0][1])\n",
    "        \n",
    "        rev_pct.append(re.findall(r'([\\.\\d]+)(?=%)', text)[0])\n",
    "        rev_total.append(re.findall(r'Revenue of.*?\\$([\\-\\.\\d]+)', text)[0])   \n",
    "        \n",
    "        rev_status_all.append(re.findall(r'(Revenue of.* (misses|beats) .+)', text)[0][0])\n",
    "        rev_status.append(re.findall(r'(Revenue of.* (misses|beats) .+)', text)[0][1])\n",
    "        \n",
    "        company.append(re.findall(r'\\((.*?)\\)', text)[0])\n",
    "\n",
    "# Create columns related to ESP\n",
    "df['esp_date'] = esp_date\n",
    "df['esp_date']= pd.to_datetime(df['esp_date'])\n",
    "\n",
    "df['esp_status_all'] = esp_status_all\n",
    "df['esp_status'] = esp_status\n",
    "df['esp'] = esp\n",
    "df['company'] = company\n",
    "\n",
    "#create columns related to YOY revenues \n",
    "df['rev_yoy_growth_percent'] = rev_pct\n",
    "df['rev_yoy_growth_percent'] = df['rev_yoy_growth_percent'].astype('float32')\n",
    "\n",
    "df['rev_status'] = rev_status\n",
    "df['rev_total_b'] = rev_total\n",
    "df['rev_total_b'] = df['rev_total_b'].astype('float32')\n",
    "\n",
    "df['rev_status_all'] = rev_status_all\n",
    "\n",
    "df = df.sort_values(by='esp_date', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get stock price and volume from Yahoo Finance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get(tickers, startdate, enddate):\n",
    "  def data(ticker):\n",
    "    return (pdr.get_data_yahoo(ticker, start=startdate, end=enddate))\n",
    "  datas = map (data, tickers)\n",
    "  return(pd.concat(datas, keys=tickers, names=['Ticker', 'Date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tickers = ['TSLA','GM','F','FCAU']\n",
    "df_df = get(tickers, datetime.datetime(2014, 1, 2), datetime.datetime(2019, 5, 13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_df['pct_price'] = df_df[\"Close\"].pct_change()\n",
    "df_df['pct_volume'] = df_df[\"Volume\"].pct_change()\n",
    "df_df['pct_price_same_day'] = (df_df ['Open'] - df_df['Close'].shift(0)) / (df_df['Close'].shift(0))\n",
    "#df_df ['weekly_return'] = (df_df ['Open'] - df_df['Close'].shift(-5)) / (df_df['Close'].shift(-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_df = df_df[['Ticker','Date','Open','Close','Volume','pct_price','pct_price_same_day','pct_volume']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.merge(df_df, left_on=['esp_date','company'], right_on=['Date','Ticker'], how='left')\n",
    "df.drop_duplicates(subset='transcripts', keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['esp_target'] = df['esp_status'].map({'misses':0, 'beats':1})\n",
    "df['rev_target'] = df['rev_status'].map({'misses':0, 'beats':1})\n",
    "df[\"esp\"] = df.esp.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['pct_price_target'] = (df['pct_price'] >0).astype(int)\n",
    "df['pct_price_target_same_day'] = (df['pct_price_same_day'] >0).astype(int)\n",
    "\n",
    "#df['weekly_return'] = (df['weekly_return'] >0).astype(int)\n",
    "\n",
    "df['volatility_percentage_return_esp'] = df.esp.pct_change()\n",
    "df['volatility_percentage_rev_yoy_growth_percent'] = df.rev_yoy_growth_percent.pct_change()\n",
    "\n",
    "df.fillna(0, inplace=True)\n",
    "df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words_target = df[['esp_date','company','transcripts', \n",
    "                      'tx_MD','tx_QA','pct_price_target', \n",
    "                      'pct_volume',\n",
    "                       'volatility_percentage_return_esp',\n",
    "                      'volatility_percentage_rev_yoy_growth_percent',\n",
    "                      'pct_price_target_same_day',\n",
    "                      'esp_target',\n",
    "                      'rev_target']]\n",
    "                      \n",
    "#df_words_target.to_csv('..data/df_transcripts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(df_words_target, open( \"../data/df_words_target.pkl\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Score and Text Stat for MD and QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "df['tx_MD_neg'] = df['tx_MD'].map(lambda x: analyser.polarity_scores(x)['neg'])\n",
    "df['tx_MD_pos'] = df['tx_MD'].map(lambda x: analyser.polarity_scores(x)['pos'])\n",
    "df['tx_MD_neu'] = df['tx_MD'].map(lambda x: analyser.polarity_scores(x)['neu'])\n",
    "df['tx_MD_compound'] = df['tx_MD'].map(lambda x: analyser.polarity_scores(x)['compound'])\n",
    "\n",
    "df['tx_QA_neg'] = df['tx_QA'].map(lambda x: analyser.polarity_scores(x)['neg'])\n",
    "df['tx_QA_pos'] = df['tx_QA'].map(lambda x: analyser.polarity_scores(x)['pos'])\n",
    "df['tx_QA_neu'] = df['tx_QA'].map(lambda x: analyser.polarity_scores(x)['neu'])\n",
    "df['tx_QA_compound'] = df['tx_QA'].map(lambda x: analyser.polarity_scores(x)['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_polarity(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "def detect_subjectivity(text):\n",
    "    return TextBlob(text).sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polarity is 0.8, which means that the statement is positive \n",
    "# 0.75 subjectivity refers that mostly it is a public opinion and not a factual information.\n",
    "df['polarity'] = df['transcripts'].map(lambda x: detect_polarity(x))\n",
    "df['subjectivity'] = df['transcripts'].map(lambda x: detect_subjectivity(x))\n",
    "\n",
    "df['tx_MD_polarity'] = df['tx_MD'].map(lambda x: detect_polarity(x))\n",
    "df['tx_QA_polarity'] = df['tx_QA'].map(lambda x: detect_polarity(x))\n",
    "\n",
    "df['tx_MD_subjectivity'] = df['tx_MD'].map(lambda x: detect_subjectivity(x))\n",
    "df['tx_QA_subjectivity'] = df['tx_QA'].map(lambda x: detect_subjectivity(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text stat in transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'transcripts'\n",
    "\n",
    "for i in range(11): \n",
    "    df[col+'_num_syl'] = [textstat.syllable_count(x) for x in list(df.loc[:,col])]\n",
    "    df[col+'_avg_sentence_length'] = [textstat.avg_sentence_length(x) for x in list(df.loc[:,col])]\n",
    "    df[col+'textstat.lexicon_count'] = [textstat.lexicon_count(x) for x in list(df.loc[:,col])]\n",
    "    df[col+'flesch_reading_ease'] = [textstat.flesch_reading_ease(x) for x in list(df.loc[:,col])]\n",
    "    df[col+'flesch_kincaid_grade'] = [textstat.flesch_kincaid_grade(x) for x in list(df.loc[:,col])]\n",
    "    df[col+'difficult_words'] = [textstat.difficult_words(x) for x in list(df.loc[:,col])]\n",
    "    df[col+'linsear_write_formula'] = [textstat.linsear_write_formula(x) for x in list(df.loc[:,col])]\n",
    "    df[col+'gunning_fog'] = [textstat.gunning_fog(x) for x in list(df.loc[:,col])]\n",
    "    df[col+'automated_readability_index'] = [textstat.automated_readability_index(x) for x in list(df.loc[:,col])]\n",
    "    df[col+'coleman_liau_index'] = [textstat.coleman_liau_index(x) for x in list(df.loc[:,col])]\n",
    "    df[col+'dale_chall_readability_score'] = [textstat.dale_chall_readability_score(x) for x in list(df.loc[:,col])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text stat in MD & QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'tx_MD'\n",
    "\n",
    "for i in range(11): \n",
    "    df[col+'_num_syl'] = [textstat.syllable_count(x) for x in list(df.loc[:,col])]\n",
    "    df[col+'_avg_sentence_length'] = [textstat.avg_sentence_length(x) for x in list(df.loc[:,col])]\n",
    "    df[col+'textstat.lexicon_count'] = [textstat.lexicon_count(x) for x in list(df.loc[:,col])]\n",
    "    df[col+'flesch_reading_ease'] = [textstat.flesch_reading_ease(x) for x in list(df.loc[:,col])]\n",
    "    df[col+'flesch_kincaid_grade'] = [textstat.flesch_kincaid_grade(x) for x in list(df.loc[:,col])]\n",
    "    df[col+'difficult_words'] = [textstat.difficult_words(x) for x in list(df.loc[:,col])]\n",
    "    df[col+'linsear_write_formula'] = [textstat.linsear_write_formula(x) for x in list(df.loc[:,col])]\n",
    "    df[col+'gunning_fog'] = [textstat.gunning_fog(x) for x in list(df.loc[:,col])]\n",
    "    df[col+'automated_readability_index'] = [textstat.automated_readability_index(x) for x in list(df.loc[:,col])]\n",
    "    df[col+'coleman_liau_index'] = [textstat.coleman_liau_index(x) for x in list(df.loc[:,col])]\n",
    "    df[col+'dale_chall_readability_score'] = [textstat.dale_chall_readability_score(x) for x in list(df.loc[:,col])]\n",
    "    \n",
    "col = 'tx_QA'\n",
    "\n",
    "for i in range(11): \n",
    "    df[col+'_num_syl'] = [textstat.syllable_count(x) for x in list(df.loc[:,col])]\n",
    "    df[col+'_avg_sentence_length'] = [textstat.avg_sentence_length(x) for x in list(df.loc[:,col])]\n",
    "    df[col+'textstat.lexicon_count'] = [textstat.lexicon_count(x) for x in list(df.loc[:,col])]\n",
    "    df[col+'flesch_reading_ease'] = [textstat.flesch_reading_ease(x) for x in list(df.loc[:,col])]\n",
    "    df[col+'flesch_kincaid_grade'] = [textstat.flesch_kincaid_grade(x) for x in list(df.loc[:,col])]\n",
    "    df[col+'difficult_words'] = [textstat.difficult_words(x) for x in list(df.loc[:,col])]\n",
    "    df[col+'linsear_write_formula'] = [textstat.linsear_write_formula(x) for x in list(df.loc[:,col])]\n",
    "    df[col+'gunning_fog'] = [textstat.gunning_fog(x) for x in list(df.loc[:,col])]\n",
    "    df[col+'automated_readability_index'] = [textstat.automated_readability_index(x) for x in list(df.loc[:,col])]\n",
    "    df[col+'coleman_liau_index'] = [textstat.coleman_liau_index(x) for x in list(df.loc[:,col])]\n",
    "    df[col+'dale_chall_readability_score'] = [textstat.dale_chall_readability_score(x) for x in list(df.loc[:,col])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(df_final, open( \"../data/df_final.pkl\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don’t try to engineer features. The NLP and ML community tried that for 50 years and success was pretty mediocre. Instead, train an LSTM or a dilated CNN. In the simplest case, use a binary cross-entropy loss with ground truth labels ‘stock went up’ and ‘stock went down’ (you’ll have to collect those yourself by checking the price before and after earnings calls). Then use back-propagation through time to train your model. Real models are of course much more complicated, but it’s a start and will likely work better than manual engineering.\n",
    "\n",
    "If you succeed, it’s still pretty unlikely that you can beat institutions in terms of time. They analyze these report in about 150ms, just to give you a benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you’re doing simple text classification, any reason why linear models wouldn’t perform well enough for a task like this? Is there any evaluation suggesting a neural network does significantly better? Also do we really need to model it as a time-series (lstm/dilated cnns)?\n",
    "\n",
    "It’s not a time-series, it’s sequential prediction. There’s absolutely no chance to obtain state-of-the-art performance with a linear model and the entire NLP literature supports this view. Many successful approaches use bidirectional LSTMs modeling n-grams nowadays. If the task is topic modeling, check out David Blei’s papers on”Latent Dirichlet Allocation” and “Dynamic topic models”. For word embeddings, “Distributed Representations of Words and Phrases and their Compositionality” is a must read.\n",
    "\n",
    "https://www.reddit.com/r/investing/comments/a1kk0p/when_applying_nlp_to_earnings_transcripts_what/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
