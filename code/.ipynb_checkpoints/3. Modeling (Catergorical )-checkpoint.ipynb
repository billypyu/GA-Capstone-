{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Class-Balance-for-Target-Value\" data-toc-modified-id=\"Class-Balance-for-Target-Value-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Class Balance for Target Value</a></span></li><li><span><a href=\"#Model-Instantiation\" data-toc-modified-id=\"Model-Instantiation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Model Instantiation</a></span></li><li><span><a href=\"#Model-prep-and-Train/Test-split\" data-toc-modified-id=\"Model-prep-and-Train/Test-split-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Model prep and Train/Test split</a></span></li><li><span><a href=\"#CountVectorizer-(Transform-text-to-vectors)\" data-toc-modified-id=\"CountVectorizer-(Transform-text-to-vectors)-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>CountVectorizer (Transform text to vectors)</a></span></li><li><span><a href=\"#Fit-Data\" data-toc-modified-id=\"Fit-Data-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Fit Data</a></span></li><li><span><a href=\"#Model-Score\" data-toc-modified-id=\"Model-Score-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Model Score</a></span></li><li><span><a href=\"#Second-Run\" data-toc-modified-id=\"Second-Run-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Second Run</a></span></li><li><span><a href=\"#Model-Score-for-MD-alone\" data-toc-modified-id=\"Model-Score-for-MD-alone-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Model Score for MD alone</a></span></li><li><span><a href=\"#Model-Score-for-QA-alone\" data-toc-modified-id=\"Model-Score-for-QA-alone-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Model Score for QA alone</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import pandas_datareader as pdr\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingRegressor, VotingClassifier, RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier,   AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import LinearSVC\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "\n",
    "from capstone_function import model_scores\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "df = pickle.load( open( \"../data/df_words_target.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Balance for Target Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.pct_price_target.value_counts(normalize=True).plot(kind='bar',title='Class Balance: Stock is up next day = 1 | Down =0');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_lemmatizer (text): \n",
    "    '''\n",
    "    Initializing tokenizer and lemmatizer to handle NLP preprocessing. \n",
    "    1. breakdown the word by alphanumeric characters and dollar with number\n",
    "    2. Create a list that appended with lemmatized posts and rejoin words by one string \n",
    "       alongside removing characters and numbers\n",
    "    '''\n",
    "    \n",
    "    tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "    tokens = [tokenizer.tokenize(post.lower()) for post in (df[text])]\n",
    "    \n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lems = []\n",
    "    for post in tokens:\n",
    "        tok_post = []\n",
    "        for word in post:\n",
    "            tok_post.append(lemmatizer.lemmatize(word)) #Remove non-letter\n",
    "            #tok_post.append(re.sub(\"[^a-zA-Z]\", \"\", lemmatizer.lemmatize(word)))\n",
    "\n",
    "        posts = \" \".join(tok_post)        \n",
    "        lems.append(posts)\n",
    "        \n",
    "    \n",
    "    words_not_used = [ 'reeve musk', 'wa', 've', 'ha', 'don']\n",
    "    \n",
    "    lems = [w for w in lems if not w in words_not_used] #stopwords.words('english')\n",
    "    \n",
    "\n",
    "    df[text] = lems #overwrite the df\n",
    "    \n",
    "    print (f'tokenizer processed: {len(tokens)}')\n",
    "    print (f'lemmatizer processed: {len(lems)}')\n",
    "    #return lems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer processed: 61\n",
      "lemmatizer processed: 61\n",
      "tokenizer processed: 61\n",
      "lemmatizer processed: 61\n",
      "tokenizer processed: 61\n",
      "lemmatizer processed: 61\n"
     ]
    }
   ],
   "source": [
    "tokenizer_lemmatizer('transcripts')\n",
    "tokenizer_lemmatizer('tx_MD')\n",
    "tokenizer_lemmatizer('tx_QA')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "logreg = LogisticRegression(random_state=42)\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "et = ExtraTreesClassifier(random_state=42)\n",
    "ada = AdaBoostClassifier(random_state=42)\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "xb = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "svm = SVC (kernel='poly', degree=2)\n",
    "clf = SVC(gamma='scale')\n",
    "svc = LinearSVC()\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "#Emsenble model\n",
    "evc = VotingClassifier( estimators=[ ('lr',logreg), ('dt', dt), ('svm', svm),('et',et) ], voting='hard')\n",
    "\n",
    "rf1 = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "\n",
    "xb1 = xgb.XGBClassifier(learning_rate =0.1,n_estimators=1000,max_depth=5,min_child_weight=1,gamma=0,subsample=0.8,colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',nthread=4,scale_pos_weight=1,seed=42)\n",
    "\n",
    "ada1 = AdaBoostClassifier (DecisionTreeClassifier(), n_estimators=10, learning_rate=1, random_state= 42 )\n",
    "\n",
    "bg = BaggingClassifier(DecisionTreeClassifier(), max_samples=0.5, max_features=1,n_estimators=10,random_state= 42 )\n",
    "\n",
    "evc1 = VotingClassifier( estimators=[ ('svc',svc),('clf',clf), ('et', et), ('xb', xb),('gb',gb) ], voting='hard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model prep and Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text for transcripts\n",
    "X = df['transcripts']\n",
    "y = df['pct_price_target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text for MD\n",
    "X_1 = df['tx_MD']\n",
    "y_1 = df['pct_price_target']\n",
    "\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_1, y_1, random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text for MD\n",
    "X_2 = df['tx_QA']\n",
    "y_2 = df['pct_price_target']\n",
    "\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2, y_2, random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer (Transform text to vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_not_use = stopwords.words('english')\n",
    "\n",
    "words_not_use.extend(\n",
    "['like', 'did', 'just', 've', 'people', 'time', 'tesla', 'feel', \n",
    " 'want', 'thing', 'kissing', 'lame', 'knock', 'knowledge', \n",
    " 'kong', 'laborious', 'laced','lacking', 'laid', 'zonei'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use default options for CountVectorizer.\n",
    "vect = CountVectorizer(analyzer = \"word\",\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = words_not_use, ngram_range=(1, 1))\n",
    "\n",
    "\n",
    "vect2 = CountVectorizer(analyzer = \"word\",\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = words_not_use, ngram_range=(2, 2))\n",
    "\n",
    "vect3 = CountVectorizer(analyzer = \"word\",\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = words_not_use, ngram_range=(3, 3))\n",
    "\n",
    "tvec = TfidfVectorizer(analyzer = \"word\",\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = words_not_use) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer\n",
    "X_train_vect = vect.fit_transform(X_train)\n",
    "X_test_vect = vect.transform(X_test)\n",
    "\n",
    "X_train_vect2 = vect2.fit_transform(X_train)\n",
    "X_test_vect2 = vect2.transform(X_test)\n",
    "\n",
    "X_train_vect3 = vect3.fit_transform(X_train)\n",
    "X_test_vect3 = vect3.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF\n",
    "X_train_vect = vect.fit_transform(X_train)\n",
    "X_test_vect = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy score</th>\n",
       "      <th>cv train score</th>\n",
       "      <th>cv test score</th>\n",
       "      <th>train score</th>\n",
       "      <th>test score</th>\n",
       "      <th>train-test gap</th>\n",
       "      <th>model status</th>\n",
       "      <th>bias vs variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.423413</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>overfit</td>\n",
       "      <td>high variance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.418849</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>-0.109722</td>\n",
       "      <td>underfit</td>\n",
       "      <td>high variance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.555754</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.229167</td>\n",
       "      <td>overfit</td>\n",
       "      <td>high variance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.315079</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.455556</td>\n",
       "      <td>overfit</td>\n",
       "      <td>high variance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.491667</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>overfit</td>\n",
       "      <td>high variance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.467857</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>overfit</td>\n",
       "      <td>high variance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.469444</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>overfit</td>\n",
       "      <td>high variance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.375992</td>\n",
       "      <td>0.377778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>overfit</td>\n",
       "      <td>high variance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.357937</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>overfit</td>\n",
       "      <td>high variance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVC(C=1.0, cache_size=200, class_weight=None, ...</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.485913</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.348611</td>\n",
       "      <td>overfit</td>\n",
       "      <td>high variance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.404167</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>overfit</td>\n",
       "      <td>high variance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.472421</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>overfit</td>\n",
       "      <td>high variance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VotingClassifier(estimators=[('svc', LinearSVC...</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.377381</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>overfit</td>\n",
       "      <td>high variance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model  accuracy score  \\\n",
       "0  LogisticRegression(C=1.0, class_weight=None, d...          0.6250   \n",
       "0  KNeighborsClassifier(algorithm='auto', leaf_si...          0.6875   \n",
       "0  (DecisionTreeClassifier(class_weight=None, cri...          0.4375   \n",
       "0  (DecisionTreeClassifier(class_weight=None, cri...          0.5000   \n",
       "0  (ExtraTreeClassifier(class_weight=None, criter...          0.5625   \n",
       "0  (DecisionTreeClassifier(class_weight=None, cri...          0.3750   \n",
       "0  ([DecisionTreeRegressor(criterion='friedman_ms...          0.6250   \n",
       "0  MultinomialNB(alpha=1.0, class_prior=None, fit...          0.6875   \n",
       "0  LinearSVC(C=1.0, class_weight=None, dual=True,...          0.6250   \n",
       "0  SVC(C=1.0, cache_size=200, class_weight=None, ...          0.5625   \n",
       "0  XGBClassifier(base_score=0.5, booster='gbtree'...          0.7500   \n",
       "0  XGBClassifier(base_score=0.5, booster='gbtree'...          0.6250   \n",
       "0  VotingClassifier(estimators=[('svc', LinearSVC...          0.6250   \n",
       "\n",
       "   cv train score  cv test score  train score  test score  train-test gap  \\\n",
       "0        0.423413       0.566667     1.000000      0.6250        0.375000   \n",
       "0        0.418849       0.433333     0.577778      0.6875       -0.109722   \n",
       "0        0.555754       0.566667     0.666667      0.4375        0.229167   \n",
       "0        0.315079       0.444444     0.955556      0.5000        0.455556   \n",
       "0        0.491667       0.433333     1.000000      0.5625        0.437500   \n",
       "0        0.467857       0.577778     1.000000      0.3750        0.625000   \n",
       "0        0.469444       0.444444     1.000000      0.6250        0.375000   \n",
       "0        0.375992       0.377778     1.000000      0.6875        0.312500   \n",
       "0        0.357937       0.633333     1.000000      0.6250        0.375000   \n",
       "0        0.485913       0.500000     0.911111      0.5625        0.348611   \n",
       "0        0.404167       0.566667     1.000000      0.7500        0.250000   \n",
       "0        0.472421       0.566667     1.000000      0.6250        0.375000   \n",
       "0        0.377381       0.566667     1.000000      0.6250        0.375000   \n",
       "\n",
       "  model status bias vs variance  \n",
       "0      overfit    high variance  \n",
       "0     underfit    high variance  \n",
       "0      overfit    high variance  \n",
       "0      overfit    high variance  \n",
       "0      overfit    high variance  \n",
       "0      overfit    high variance  \n",
       "0      overfit    high variance  \n",
       "0      overfit    high variance  \n",
       "0      overfit    high variance  \n",
       "0      overfit    high variance  \n",
       "0      overfit    high variance  \n",
       "0      overfit    high variance  \n",
       "0      overfit    high variance  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm = pd.DataFrame() # Instantiate the empty shell to hold the function\n",
    "models = [logreg, knn, bg, rf, et, ada, gb, nb, svc, clf, xb,xb1,evc]\n",
    "\n",
    "for i in models:\n",
    "    sm = sm.append(model_scores(i, X_train_vect, y_train, X_test_vect, y_test))\n",
    "sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.predict(X_test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.predict(X_test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbp = xb.predict_proba(X_test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfp= rf.predict_proba(X_test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.95667565, 0.04332433],\n",
       "       [0.50844026, 0.49155977],\n",
       "       [0.22949928, 0.7705007 ],\n",
       "       [0.8892762 , 0.1107238 ],\n",
       "       [0.7464902 , 0.25350982],\n",
       "       [0.66638744, 0.33361256],\n",
       "       [0.40718162, 0.5928184 ],\n",
       "       [0.88888186, 0.11111813],\n",
       "       [0.57492834, 0.42507166],\n",
       "       [0.9146126 , 0.08538741],\n",
       "       [0.38623613, 0.61376387],\n",
       "       [0.5141312 , 0.48586878],\n",
       "       [0.2458536 , 0.7541464 ],\n",
       "       [0.61025333, 0.38974667],\n",
       "       [0.63365376, 0.36634624],\n",
       "       [0.23593885, 0.76406115]], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7, 0.3],\n",
       "       [0.4, 0.6],\n",
       "       [0.6, 0.4],\n",
       "       [0.8, 0.2],\n",
       "       [0.6, 0.4],\n",
       "       [0.6, 0.4],\n",
       "       [0.4, 0.6],\n",
       "       [0.6, 0.4],\n",
       "       [0.3, 0.7],\n",
       "       [0.7, 0.3],\n",
       "       [0.3, 0.7],\n",
       "       [0.5, 0.5],\n",
       "       [0.7, 0.3],\n",
       "       [0.7, 0.3],\n",
       "       [0.5, 0.5],\n",
       "       [0.4, 0.6]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.82833782, 0.17166216],\n",
       "       [0.45422013, 0.54577989],\n",
       "       [0.41474964, 0.58525036],\n",
       "       [0.8446381 , 0.1553619 ],\n",
       "       [0.67324509, 0.32675491],\n",
       "       [0.63319372, 0.36680628],\n",
       "       [0.40359081, 0.59640919],\n",
       "       [0.74444093, 0.25555907],\n",
       "       [0.43746417, 0.56253583],\n",
       "       [0.8073063 , 0.1926937 ],\n",
       "       [0.34311807, 0.65688193],\n",
       "       [0.50706559, 0.49293439],\n",
       "       [0.4729268 , 0.5270732 ],\n",
       "       [0.65512667, 0.34487333],\n",
       "       [0.56682688, 0.43317312],\n",
       "       [0.31796942, 0.68203058]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xbp + rfp)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy score</th>\n",
       "      <th>cv train score</th>\n",
       "      <th>cv test score</th>\n",
       "      <th>train score</th>\n",
       "      <th>test score</th>\n",
       "      <th>train-test gap</th>\n",
       "      <th>model status</th>\n",
       "      <th>bias vs variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.444048</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>overfit</td>\n",
       "      <td>high variance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.423413</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>overfit</td>\n",
       "      <td>high variance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model  accuracy score  \\\n",
       "0  LogisticRegression(C=1.0, class_weight=None, d...          0.8125   \n",
       "0  XGBClassifier(base_score=0.5, booster='gbtree'...          0.3125   \n",
       "\n",
       "   cv train score  cv test score  train score  test score  train-test gap  \\\n",
       "0        0.444048       0.566667          1.0      0.8125          0.1875   \n",
       "0        0.423413       0.555556          1.0      0.3125          0.6875   \n",
       "\n",
       "  model status bias vs variance  \n",
       "0      overfit    high variance  \n",
       "0      overfit    high variance  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm = pd.DataFrame() # Instantiate the empty shell to hold the function\n",
    "models = [logreg, xb]\n",
    "\n",
    "for i in models:\n",
    "    sm = sm.append(model_scores(i, X_train_vect2, y_train, X_test_vect2, y_test))\n",
    "sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy score</th>\n",
       "      <th>cv train score</th>\n",
       "      <th>cv test score</th>\n",
       "      <th>train score</th>\n",
       "      <th>test score</th>\n",
       "      <th>train-test gap</th>\n",
       "      <th>model status</th>\n",
       "      <th>bias vs variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.396627</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>overfit</td>\n",
       "      <td>high variance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.621230</td>\n",
       "      <td>0.455556</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>overfit</td>\n",
       "      <td>high variance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model  accuracy score  \\\n",
       "0  LogisticRegression(C=1.0, class_weight=None, d...          0.6875   \n",
       "0  XGBClassifier(base_score=0.5, booster='gbtree'...          0.3750   \n",
       "\n",
       "   cv train score  cv test score  train score  test score  train-test gap  \\\n",
       "0        0.396627       0.511111          1.0      0.6875          0.3125   \n",
       "0        0.621230       0.455556          1.0      0.3750          0.6250   \n",
       "\n",
       "  model status bias vs variance  \n",
       "0      overfit    high variance  \n",
       "0      overfit    high variance  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm = pd.DataFrame() # Instantiate the empty shell to hold the function\n",
    "models = [logreg,xb]\n",
    "\n",
    "for i in models:\n",
    "    sm = sm.append(model_scores(i, X_train_vect3, y_train, X_test_vect3, y_test))\n",
    "sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy score</th>\n",
       "      <th>cv train score</th>\n",
       "      <th>cv test score</th>\n",
       "      <th>train score</th>\n",
       "      <th>test score</th>\n",
       "      <th>train-test gap</th>\n",
       "      <th>model status</th>\n",
       "      <th>bias vs variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.423413</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>overfit</td>\n",
       "      <td>high variance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.418849</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>-0.109722</td>\n",
       "      <td>underfit</td>\n",
       "      <td>high variance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.460516</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.518056</td>\n",
       "      <td>overfit</td>\n",
       "      <td>high variance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.554365</td>\n",
       "      <td>0.311111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>overfit</td>\n",
       "      <td>high variance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.467857</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>overfit</td>\n",
       "      <td>high variance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.469444</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>overfit</td>\n",
       "      <td>high variance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.375992</td>\n",
       "      <td>0.377778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>overfit</td>\n",
       "      <td>high variance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.357937</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>overfit</td>\n",
       "      <td>high variance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVC(C=1.0, cache_size=200, class_weight=None, ...</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.485913</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.348611</td>\n",
       "      <td>overfit</td>\n",
       "      <td>high variance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.404167</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>overfit</td>\n",
       "      <td>high variance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VotingClassifier(estimators=[('svc', LinearSVC...</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.377381</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>overfit</td>\n",
       "      <td>high variance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model  accuracy score  \\\n",
       "0  LogisticRegression(C=1.0, class_weight=None, d...          0.6250   \n",
       "0  KNeighborsClassifier(algorithm='auto', leaf_si...          0.6875   \n",
       "0  (DecisionTreeClassifier(class_weight=None, cri...          0.4375   \n",
       "0  (ExtraTreeClassifier(class_weight=None, criter...          0.3750   \n",
       "0  (DecisionTreeClassifier(class_weight=None, cri...          0.3750   \n",
       "0  ([DecisionTreeRegressor(criterion='friedman_ms...          0.6250   \n",
       "0  MultinomialNB(alpha=1.0, class_prior=None, fit...          0.6875   \n",
       "0  LinearSVC(C=1.0, class_weight=None, dual=True,...          0.6250   \n",
       "0  SVC(C=1.0, cache_size=200, class_weight=None, ...          0.5625   \n",
       "0  XGBClassifier(base_score=0.5, booster='gbtree'...          0.7500   \n",
       "0  VotingClassifier(estimators=[('svc', LinearSVC...          0.6250   \n",
       "\n",
       "   cv train score  cv test score  train score  test score  train-test gap  \\\n",
       "0        0.423413       0.566667     1.000000      0.6250        0.375000   \n",
       "0        0.418849       0.433333     0.577778      0.6875       -0.109722   \n",
       "0        0.460516       0.577778     0.955556      0.4375        0.518056   \n",
       "0        0.554365       0.311111     1.000000      0.3750        0.625000   \n",
       "0        0.467857       0.577778     1.000000      0.3750        0.625000   \n",
       "0        0.469444       0.444444     1.000000      0.6250        0.375000   \n",
       "0        0.375992       0.377778     1.000000      0.6875        0.312500   \n",
       "0        0.357937       0.633333     1.000000      0.6250        0.375000   \n",
       "0        0.485913       0.500000     0.911111      0.5625        0.348611   \n",
       "0        0.404167       0.566667     1.000000      0.7500        0.250000   \n",
       "0        0.377381       0.633333     1.000000      0.6250        0.375000   \n",
       "\n",
       "  model status bias vs variance  \n",
       "0      overfit    high variance  \n",
       "0     underfit    high variance  \n",
       "0      overfit    high variance  \n",
       "0      overfit    high variance  \n",
       "0      overfit    high variance  \n",
       "0      overfit    high variance  \n",
       "0      overfit    high variance  \n",
       "0      overfit    high variance  \n",
       "0      overfit    high variance  \n",
       "0      overfit    high variance  \n",
       "0      overfit    high variance  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm = pd.DataFrame() # Instantiate the empty shell to hold the function\n",
    "models = [logreg, knn, rf, et, ada, gb, nb, svc, clf, xb,evc]\n",
    "\n",
    "for i in models:\n",
    "    sm = sm.append(model_scores(i, X_train_vect, y_train, X_test_vect, y_test))\n",
    "sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Score for MD alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['tx_MD']\n",
    "y = df['pct_price_target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y)\n",
    "\n",
    "\n",
    "# Create document-term matrices.\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "# Use Naive Bayes to predict the star rating.\n",
    "\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "\n",
    "# Calculate accuracy.\n",
    "print((metrics.accuracy_score(y_test, y_pred_class)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Score for QA alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n",
    "from sklearn import metrics\n",
    "\n",
    "# Use default options for CountVectorizer.\n",
    "vect = CountVectorizer(analyzer = \"word\",\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = 'english', ngram_range=(2, 2))\n",
    "\n",
    "\n",
    "X = df['tx_QA']\n",
    "y = df['pct_price_target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y)\n",
    "\n",
    "\n",
    "# Create document-term matrices.\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "# Use Naive Bayes to predict the star rating.\n",
    "\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "\n",
    "# Calculate accuracy.\n",
    "print((metrics.accuracy_score(y_test, y_pred_class)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
